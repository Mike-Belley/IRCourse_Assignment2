{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "96x8oNdxfrX_",
        "jZr6E258fUGy",
        "c-7OQl57fj5C",
        "zQi6unVCt0to",
        "lRK7vXFy4BYa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "96x8oNdxfrX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import string\n",
        "\n",
        "import csv\n",
        "\n",
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztckfrQwUuAa",
        "outputId": "45b149ef-a306-4a40-c276-f83ec0b4e24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processs the data into an inverted index"
      ],
      "metadata": {
        "id": "jZr6E258fUGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data():\n",
        "  # initialize dictionary\n",
        "  words_dict = {}\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # go thru all the questions\n",
        "  for question_id in post_reader.map_questions:\n",
        "    question = post_reader.map_questions[question_id]\n",
        "\n",
        "    # tokenize the title and body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(question.title)\n",
        "    word_tokens += word_tokenize(question.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # go thru all the strings in filtered_text\n",
        "    for word in filtered_text:\n",
        "      # if the word is already in the dictionary\n",
        "      if word in words_dict:\n",
        "        # if the questionid for the word is there, increment\n",
        "        if question_id in words_dict[word]:\n",
        "          words_dict[word][question_id] += 1\n",
        "\n",
        "        # if the questionid for the word is not there, initialize to 1\n",
        "        else:\n",
        "          words_dict[word][question_id] = 1\n",
        "\n",
        "      # if the word is not in the dictionary, initialize it\n",
        "      else:\n",
        "        words_dict[word] = {question_id: 1}\n",
        "  \n",
        "  # go thru all the answers\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    answer = post_reader.map_just_answers[answer_id]\n",
        "\n",
        "    # tokenize the body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(answer.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # go thru all the strings in filtered_text\n",
        "    for word in filtered_text:\n",
        "      # if the word is already in the dictionary\n",
        "      if word in words_dict:\n",
        "        # if the answerid for the word is there, increment\n",
        "        if answer_id in words_dict[word]:\n",
        "          words_dict[word][answer_id] += 1\n",
        "\n",
        "        # if the answerid for the word is not there, initialize to 1\n",
        "        else:\n",
        "          words_dict[word][answer_id] = 1\n",
        "\n",
        "      # if the word is not in the dictionary, initialize it\n",
        "      else:\n",
        "        words_dict[word] = {answer_id: 1}\n",
        "      \n",
        "  return words_dict\n",
        "  \n",
        "words_dict = process_data()\n",
        "# for key, value in words_dict.items():\n",
        "#     print(key, ' : ', value)"
      ],
      "metadata": {
        "id": "8zk8-zQaSJjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term at a Time Ranking"
      ],
      "metadata": {
        "id": "c-7OQl57fj5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a search and print the top results\n",
        "def search_query(search_text):\n",
        "\n",
        "  # split the search into its terms\n",
        "  search_words = search_text.split()\n",
        "\n",
        "  # check if all the words are in the dictionary\n",
        "  check = True\n",
        "  if len(search_words) == 0:\n",
        "    check = False\n",
        "  for word in search_words:\n",
        "    if word not in words_dict.keys():\n",
        "      check = False\n",
        "\n",
        "  if check == False:\n",
        "    print(\"No Valid Searches for: \" + search_text)\n",
        "  else:\n",
        "    # initialize a dictionary to the inverted index entry of the first word in the search\n",
        "    search_docs_rank = words_dict[search_words[0]]\n",
        "\n",
        "    # go thru the rest of the words in the search, if they exist\n",
        "    if len(search_words) > 1:\n",
        "\n",
        "      # the goal here is to intersect the keys, while adding the values\n",
        "      # this ensures that only documents with all of the search terms included will be returned\n",
        "      for word in search_words[1:]:\n",
        "        # declare a temporary dictionary\n",
        "        temp_dict = {}\n",
        "\n",
        "        # go thru the id in the current word\n",
        "        for key, value in words_dict[word].items():\n",
        "          # if the id is also in the main dictionary, add it to the temp one\n",
        "          if key in search_docs_rank:\n",
        "            temp_dict[key] = value + search_docs_rank[key]\n",
        "        # redeclare the main dict to the temp one\n",
        "        search_docs_rank = temp_dict\n",
        "      \n",
        "    # print(search_docs_rank)\n",
        "    # order by the values\n",
        "    sorted_by_value = dict(sorted(search_docs_rank.items(), key=lambda item: item[1], reverse=True))\n",
        "    print(\"Search Results for: \" + search_text)\n",
        "    for i in range(1,11):\n",
        "      print(str(i) + \": \" + str(list(sorted_by_value.keys())[i-1]) + \" with \" + str(list(sorted_by_value.values())[i-1]) + \" Occurences\")\n",
        "      \n",
        "  \n",
        "search_query(\"persian coffee\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfp0twA8fozM",
        "outputId": "ae467dd8-164c-452e-f59b-72a490412540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Valid Searches for: persian coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "Search Results for: espresso\n",
        "\n",
        "1: 3269 with 33 Occurences: Relevant\n",
        "\n",
        "2: 1574 with 23 Occurences: Relevant\n",
        "\n",
        "3: 2095 with 20 Occurences: Relevant\n",
        "\n",
        "4: 283 with 14 Occurences: Not Relevant\n",
        "\n",
        "5: 2077 with 13 Occurences: Relevant\n",
        "\n",
        "6: 5537 with 13 Occurences: Relevant\n",
        "\n",
        "7: 2087 with 12 Occurences: Relevant\n",
        "\n",
        "8: 2116 with 12 Occurences: Relevant\n",
        "\n",
        "9: 3721 with 12 Occurences: Relevant\n",
        "\n",
        "10: 3438 with 11 Occurences: Not Relevant\n",
        "\n",
        "---------------------------------------------------\n",
        "\n",
        "Search Results for: turkish coffee\n",
        "\n",
        "1: 2392 with 65 Occurences: Relevant\n",
        "\n",
        "2: 1833 with 19 Occurences: Relevant\n",
        "\n",
        "3: 4407 with 19 Occurences: Not Relevant\n",
        "\n",
        "4: 4185 with 17 Occurences: Not Relevant\n",
        "\n",
        "5: 4273 with 17 Occurences: Not Relevant\n",
        "\n",
        "6: 5095 with 16 Occurences: Relevant\n",
        "\n",
        "7: 3101 with 15 Occurences: Not Relevant\n",
        "\n",
        "8: 165 with 13 Occurences: Not Relevant\n",
        "\n",
        "9: 2379 with 13 Occurences: Relevant\n",
        "\n",
        "10: 2647 with 13 Occurences: Not Relevant\n",
        "\n",
        "---------------------------------------------------\n",
        "\n",
        "No Valid Searches for: persian coffee\n",
        "\n"
      ],
      "metadata": {
        "id": "MtBI29wft4nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions"
      ],
      "metadata": {
        "id": "zQi6unVCt0to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A**\n",
        "\n",
        "Measure|espresso | turkish coffee | persian coffee\n",
        "---------|--------|--------------|------------\n",
        "Precision @ 10|0.8 | 0.4 | 0 (No documents recalled)\n",
        "\n",
        "--------------------------------------\n",
        "**Part B**\n",
        "\n",
        "For Espresso, the precision in assignment 1 was 0.3, the precision in this assignment is 0.8, a big improvement. i believe that this is because this method, when using a single search word, will return the document that talks about that one thing the most.\n",
        "\n",
        "For Turkish Coffee, the precision in assignment 1 was 0.6, the precision in this assignment is 0.4, a significant drop in precision. I believe that this is because, using this method, \"coffee\" is as equally important in ranking as \"turkish\". This matters because this method could return a post that mentions coffe in general a ton of times, but turkish coffee only once, which is what ended up happening when I looked through the returned documents.\n",
        "\n",
        "For Persian Coffee, the precision in both assignment was 0. This is because there are no posts with \"persian\" in the collection.\n",
        "\n",
        "Overall, I noticed that there were more answers returned than questions this time. This should lead to better results since its hard to find an answer to a query in a question"
      ],
      "metadata": {
        "id": "zhxT9Kh2uqlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export to TSV\n"
      ],
      "metadata": {
        "id": "lRK7vXFy4BYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('coffee_index.tsv', 'w') as f:\n",
        "  for word in words_dict.keys():\n",
        "    f.write(\"%s\"%(word))\n",
        "    for key,value in words_dict[word].items():\n",
        "      key = str(key)\n",
        "      value = str(value)\n",
        "      f.write(\"\\t%s:%s\"%(key,value))\n",
        "\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "ImONagaezLpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}